****前提处境**：**

~~~
	大四毕业，已经准备继续读研。本科扎实地学完了计算机基础课程，仅有大三暑假在某公司的两个月研发部实习经历，具体只是利用Python+shell编写了一个针对waf新版本数据的自动化测试脚本，基本没有深入学习什么内容，有点偏数据分析的感觉。
	毕业想搞研发，不知道怎么入手，没有方向。
~~~

 跟学长简单交流了一下，总结如下。



**方向选择**

1. 算法：阅读论文，研究算法，将算法落地
2. 架构：整体设计，就像摩天大楼的地基
3. 业务(产品)：注重产品需求和创意，实际需要实现的功能



**以下推荐内容仅针对和我情况相似的同学**

1. 对于导师侧，也就是在学校内需要做的事情，比如老师的项目，比赛，科研等都认真对待，不要不当回事；自己侧，如果自己想学的东西跟老师安排的东西产生冲突，不要抗拒，老老实实做老师给的东西，在额外的时间内学习想学的内容；因为最后你可能会发这两方面可能是互通的。

2. 自己的空余时间做些什么

   1. 理论
      1. 算法-->刷题，通过做题抽象出来，归纳总结，要知道这一题是属于哪一类，比如是动态规划。如果刷题时自己没做出来，去看解答，思考自己是由于什么根本原因没有解决，根据原因总结分类。
      2. 计算机网络：一定的基础学习，再结合2.应用中的实际情况深入学习
      3. 数据库：一定的基础学习，再结合2.应用中的实际情况深入学习
      4. 操作系统：一定的基础学习，再结合2.应用中的实际情况深入学习
   2. 应用
      1. 阶段一：实际地去使用各种应用层面地技术，不需要太关注底层实现原理，只需不断练习直到熟练掌握**编程语言/Linux/数据库/HTTP网络协议**。
      2. 阶段二：强化理论，将之前理论学习和实际软件应用场景结合，这样深挖理论地时候会快速构建起基础图谱。不会有单独学习理论但不知道在何处使用地无力感。
      3. 阶段三：实际项目实践。选择一个方向研究，尝试做一个实际的项目。

3. 时间规划：

   考虑到研究生临近毕业阶段的实习+找工作+论文事项，实际留给我沉淀的时间大概在一年半左右(专硕两年半培养)。需要在一年半的空闲时间内完成**理论+应用**的学习，具体每一个阶段需要根据难度和具体情况来制定。

   

## 阶段一

### 设计题目

1. 根据自己喜欢的站点类型如（知乎、博客、微博等），用python开发一个迷你定向爬取器，实现对种子链接[广度优先]抓取，并把URL符合特别规定【规则】图片地址（绝对路径）保存到数据库，具体数据结构设计可自行设计。
2. 实现简易可供访问的web站点，主要实现根据搜索词，就能找到相关文章。

### 设计要求

1. 完整的设计文档
2. 注意简单的代码和日志规范，能比较好的回溯问题

### 设计框架

![设计框架](.\设计框架.png)

### 设计模块

注：从上往下是学习顺序

#### MySQL

1. 在Linux下安装好MySQL，并进行增删改查的基本操作使用练习
2. 使用Python编写代码在数据库中进行增删改查操作

##### 实践操作

1. 实验设备

   1. 操作系统：kali2，
   2. MySQL：Ver 15.1 Distrib 10.3.12-MariaDB, for debian-linux-gnu (x86_64) using readline 5.2
   3. Python：3.6.8
   4. pymysql：0.10.0

2. 学习基本操作

   1. 使用Python连接数据库

      ~~~Python
      import pymysql
      
      # 创建连接
      con = pymysql.connect(host='127.0.0.1', port=3306, user='root', password='passwd', database='test')
      print(con)
      # 在这一步遇到两个错误，1是pip install pymysql安装的pymysql默认是安装在kali自带的Python2下面的，所以需要将/usr/local/lib/python2.7/dist-packages下的文件拷贝到Python3.6下
      # 2是pymysql.err.OperationalError: (1698, "Access denied for user 'root'@'localhost'")，解决方法是update mysql.user set plugin='mysql_native_password' where user='root';原本的plugin字段是unix_socket，而使用密码登录的mysql对应的字段应该是mysql_native_password；
      ~~~

   2. 创建游标对象

      ~~~python
      cur=con.cursor()
      ~~~

   3. 创建表的SQL

      ~~~python
      	sql = """
      		create table student(
      			Sno int primary key,
      			Sname varchar(30) not null,
      			age int(2),
      			score int
      		)
      	"""
      ~~~

   4. 执行SQL语句

      ~~~Python
      	try:
      		cur.execute(sql)
      		print(创建表成功)
      	except Exception as e:
      		print(e)
      		print('创建表失败')
      		logging.error(str(e))
      	finally:
      		# 关闭连接
      		con.close()
      ~~~

   5. 向表里插入数据

      ~~~Python
      	# 插入数据
      	sql = 'insert into student(Sno, Sname, age, score) values(%s,%s,%s,%s)'
      	try:
      		cur.execute(sql,(1,'小明', 20, 90))
      		con.commit()
      		print('插入成功')
      	except Exception as e:
      		print(e)
      		con.rollback()
      		print('插入失败')
      		logging.error(str(e))
      	finally:
      		con.close()
      ~~~

   6. 查询数据

      ~~~Python
      	# 查询数据
      	sql = 'select * from student where score >= 90'
      	try:
      		cur.execute(sql)
      		students = cur.fetchall() # 取出所有符合条件的查询结果
      		for student in students:
      			print(student)
      	except Exception as e:
      		print(e)
      		print('查询失败')
      		logging.error(str(e))
      	finally:
      		con.close()
      ~~~

   7. 更新数据
   
      ~~~Python
      	# 修改数据
      	sql = 'update student set Sname=%s where Sno = %s '
      	try:
      		cur.execute(sql, ('小花'， 1))
      		con.cimmit()
      		print('修改成功')
      	except Exception as e:
      		print(e)
      		print('修改失败')
      		logging.error(str(e))
      	finally:
      		con.close()
      ~~~
   
   8. 删除数据
   
      ~~~Python
      	# 删除数据
      	sql = 'delete from student where Sname = %s'
      	try:
      		cur.execute(sql, ('小花'))
      		con.commit()
      		print('删除成功')
      	except Exception as e:
      		print(e)
      		print('删除失败')
      		logging.error(str(e))
      	finally:
      		con.close()
      ~~~
   
      



#### 前端

1. HTML+CSS，实现一个简单的呈现网页

##### 实践操作

1. HTML+CSS实现一个简单的搜索界面index.html

   ~~~html
   <!DOCTYPE html>
   <html>
   <head>
   	<title>test</title>
   </head>
   <body>
   
   	<form action="" method="get">
   		<p><input type="text" name="" placeholder="请输入搜索内容"></p>
   		<p><input type="submit" name="" value="搜索"></p>
   	</form>
   </body>
   </html>
   ~~~



#### webserver

1. 学习Python flask即可，向服务器发起请求，获得响应，解析数据包，提取关键数据(这个主要是为了提取之后的爬虫信息，用于在MySQL中存储)
2. 接收前端传进来的数据包进行解析，提取关键内容，用于在数据库中查找对应内容

##### 实践操作

1. flask框架

   ~~~Python
   from flask import Flask
   
   app = Flask(__name__)
   
   @app.route('/')
   def hello_world():
   	return 'hello world'
   
   
   if __name__ == '__main__':
   	app.run(host=0.0.0.0, port=8080, debug=true)
       # host设置为0.0.0.0时，外网也可以通过我本机的ip地址来访问
       # debug=true，在代码中修改，服务器会实时reload，刷新网页会相应发生变化，不用关闭程序在重启
   ~~~

   ~~~Python
   from flask import Flask
   
   app = Flask(__name__)
   
   @app.route('/')
   def smile():
   	return '哈哈哈哈哈哈'
   
   @app.route('/index')
   def index():
   	return '哈哈哈哈哈哈'
   
   if __name__ == '__main__':
   	app.run(port=5000)
   # 访问路由时，是按代码中从上到下的路由进行匹配访问
   ~~~

   运行以上两个代码程序，就会在本地的5000端口和8080端口对应程序返回相应的内容，http://127.0.0.1:5000是为了找主机地址，'/'是路由，会去找服务器上5000端口下的'/'路由

   ![flask框架](C:\Users\63538\Desktop\开发\flask框架.png)

2. 请求request

   - 包括请求行、请求头、请求体，请求行：请求地址、请求方法(get、post)，请求头：是一个**key：value**的形式(在爬虫部分起重要作用)
   - from flask import request，可以获得request的headers、path等等参数

3. 响应response

   - 包括响应行、响应头、响应体，响应行：状态码200 404等，响应头：也是key：value的形式显示，比如content-length：45，响应体：服务器想让用户看到的东西，响应的内容是交给浏览器去解释的，所以Python代码中的标签都能被浏览器解释，从而显示给用户
   - Response是一个类，类可以调用很多方法，比如set_cookie等
   - Python文件可与HTML文件交互

4. 网页HTML文件与Python文件进行交互

   HTML文件必须在templates文件夹中，render利用模板引擎，找到模板文件夹中的index.html文件，将index.html中的内容转成字符串str，将这个字符串返回给客户端，交流浏览器解释显示

   ![render作用](C:\Users\63538\Desktop\开发\render作用.png)

   ~~~Python
   from flask import Flask
   
   app = Flask(__name__)
   
   @app.route('/search')
   def search():
   	r = render_template('index.html') # 之前实现的简单搜索页面
   	return r
   
   
   if __name__ == '__main__':
   	app.run(port=8080)
   ~~~

5. 前端数据与后端交互

   index.html

   ~~~HTML
   <!DOCTYPE html>
   <html>
   <head>
   	<title>test</title>
   </head>
   <body>
   	<h1>寻找你所需的内容</h1>
   	<div>
   		<form action="/result" method="get">
   		<!-- 点击搜索按钮，也就是type=submit的内容，就会默认执行action动作，这里表示跳转到/result界面-->
   		<!--如果是表单提交，则必须在表单的元素上添加name=""，才能在后台拿到这个对应的值-->	
   			<p><input type="text" name="searchText" placeholder="请输入搜索内容"></p>
   			<p><input type="submit" value="搜索"></p>
   		</form>
   	</div>
   </body>
   </html>
   ~~~

   **search.py**

   ~~~python
   from flask import Flask
   
   app = Flask(__name__)
   
   @app.route('/search')
   def search():
   	r = render_template('index.html') # 之前实现的简单搜索页面，与search.py在同一目录下建一个templates文件夹，注意必须要小写。在windows下使用Pycharm大写Templates文件夹也行，但是在Linux下会找不到文件
   	return r
   
   
   # 点击搜索后，跳转的页面；
   @app.route('/result', methods=['GET', 'POST']) # 如果使用post方法提交，需要声明methods参数
   def showResult():
   	print(request.args) # 打印前端传进后台的参数,类型是类似字典，args只能接收到GET方式传参的参数
   	print(request.args.get('searchText')) # 打印前端想要搜索的内容
   
   	print(request.form) # 如果请求方法是post，则需要通过这种形式取前端提交的值
   	print(request.form.get('searchText')) # 打印前端想要搜索的内容
   
   if __name__ == '__main__':
   	app.run(port=8080)
   
   ~~~

   结果展示界面
   
   **Jinja2**模板引擎使用以下分隔符从HTML转义。
   
   - {% ... %}用于语句
   
   - {{ ... }}用于表达式可以打印到模板输出
   
   - {# ... #}用于未包含在模板输出中的注释
   
   - \# ... ##用于行语句
   
     **result.html**
   
   ~~~html
   {% block title %}搜索结果{% endblock %}
   
   {% block page_content %}
   	<table border=1>
   	<tr>
   		<th>学号</th>
   		<th>姓名</th>
   		<th>年龄</th>
   		<th>成绩</th>
   	</tr>
   		{% for i in result %}
   			<tr>
   				<td>{{ i[0] }}</td>
   				<td>{{ i[1] }}</td>
   				<td>{{ i[2] }}</td>
   				<td>{{ i[3] }}</td>
   			</tr>
   		{% endfor %}
   	</table>
   {% endblock %}
   ~~~

##### 总结

​	至此，已经实现从前端的搜索框向后端服务器传递搜索的关键词，后端服务器通过这个关键词从数据库中查询满足条件的内容，再通过render_tempale()渲染到前端的浏览器界面。

​	例如搜索学号为1的学生信息，数据库存储的是学生表包括学号、姓名、年龄、成绩。![image-20200731221817856](.\image-20200731221817856.png)

![搜索结果](.\搜索结果.png)

#### 爬虫

1. 这个模块是离线计算，我先爬取好内容并存储好之后再进行实时的搜索
2. 学习Python的并发编程、网络编程、正则等，发送请求并获得响应，解析网页内容，抽取需要的特定的内容并封装成所需结构体，用于在MySQL中存储
3. 根据结构体设计具体的MySQL表结构，将所需内容存储到MySQL表中。
4. 此次题目不考察数据量，爬取的数据量可根据自己喜好来定

##### 实践操作

1. 爬取内容：豆瓣电影Top250的基本信息，包括电影名称、豆瓣评分、评价数、电影概况

2. 爬虫原理：模拟浏览器打开页面

3. 相关库：bs4 网页解析，获取数据；re 正则表达式，进行文字匹配； urllib 制定url，获取网页数据

4. 流程：爬取网页-->解析数据-->-保存数据

5. 爬虫的header模拟浏览器访问的header，否则会被识别爬虫

6. urllib模块，获取网页数据

   ~~~Python
   def askURL(url):
       # 设置UA
       head = {
           "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.105 Safari/537.36"
       }
       request = urllib.request.Request(url, headers=head)
       html = ""
       try:
           response =urllib.request.urlopen(request)
           html = response.read().decode("utf-8")
           print(html)
       except Exception as e:
           print(e)
       return html
   ~~~

7. beautifulSoup4将复杂的HTML文档转换成一个复杂的树形结构，每个节点都是Python对象

   1. Tag，标签及其内容，拿到第一个找到的标签

      ~~~python
      file = open('./baidu.html', 'rb')
      html = file.read()
      bs = BeautifulSoup(html, "html.parser")
      print(bs.title)
      print(type(bs.title))
      输出：
      <title>百度一下你就知道</title>
      <class 'bs4.element.Tag'>
      ~~~

   2. NavigableString 标签里的内容(字符串)

      ~~~Python
      print(bs.title.string)
      print(type(bs.title.string))
      输出：
      百度一下，你就知道
      <class 'bs4.element.NavigableString'>
      ~~~

   3. 标签的属性

      ~~~python
      print(bs.a.attrs)
      输出：
      {'class':['mnav'],'href':'http://news.baidu.com','name':'tj_trnews'}
      ~~~

   4. 整个文档

      ~~~Python
      print(type(bs))
      输出：
      <'class': BeautifulSoup>
      ~~~

   5. Comment，是一个特殊的NavigableString ，输出的内容不包含注释

   6. 文档搜索

      1. find_all()，查找与给定字符完全一致的内容

         ~~~python
         bs.find_all("a")
         ~~~

         正则表达式

         ~~~Python
         bs.find_all(re.compile("a"))
         ~~~

         属性参数

         ~~~python
         bs.find_all(id="head")
         ~~~

         text参数

         ~~~python
         bs.find_all(text="hao123")
         bs.find_all(text=["hao123","地图"]
         bs.find_all(text=re.compile("\d")) # 标签里的字符串
         ~~~

      2. CSS选择器

         ~~~python
         print(bs.select('title')) # 通过标签来查找
         输出：<title>百度一下你就知道</title>
         ~~~

         ~~~python
         print(bs.select('.mnac')) # 通过类名来查找
         print(bs.select('#u1')) # 通过id来查找
         print(bs.select('a[class="bri"]')) # 通过属性来查找
         print(bs.select('.mnac')) # 通过类名来查找
         ~~~

   7. 正则表达式

   

   

### 模块部署

1. Linux操作系统：webserver + MySQL
2. 爬虫代码也运行在Linux服务器上
3. 使用虚拟机搭建服务器

### 结果

​	具体内容：爬取豆瓣TOP250部电影的相关内容，根据用户的前端输入显示对应的排名电影给用户

1.  搜索界面

   ![search](.\search.png)

2. 结果界面

   ![res](.\res.png)

### 学习途径

1. 尚学堂Python400集大型视频(B站也有)，包括Python与MySQL连接、并发编程、网络编程、正则、爬虫基础
2. flask+MySQL，flask web搭建开发环境等



### 阶段总结

**webserver**

现在在虚拟机上开终端跑search.py，其实是开启了一个进程来跑这个Python程序，**这个进程所分配到的系统资源是有限的**，如果前端网页访问流量过大，这个进程就挂了，网页服务就断了。怎么样可以让网页服务不那么容易挂掉呢，这时就需要一个webserver，webserver可以根据流量分配系统资源，流量大，资源就分配的多，网页服务就可以稳定的存在。有了webserver，就可以让开发者更加专注地注重后端代码的开发，而不需要担心流量过多等其他原因导致服务挂掉的问题。

后端程序，apache 是webserver的一种，可以动态的调配操作系统的资源；python文件跑在webserver上面；具有了更强大的处理能力；apache相当于一个容器，调度流量，一台服务器的资源有限，当流量过高时，webserver会进行流量的分配，以便更好地利用系统资源。

apache和NGINX，两者最核心的区别在于 apache 是同步多进程模型，一个连接对应一个进程，而 nginx 是异步的，多个连接（万级别）可以对应一个进程 (推荐学习：apache使用）

一般来说，需要性能的 web 服务，用 nginx 。如果不需要性能只求稳定，更考虑 apache ，后者的各种功能模块实现得比前者好，例如 ssl 的模块就比前者好，可配置项多。

**虚拟机搭一个apache，把search.py跑在apache上。**



## 阶段二

1. 爬虫模块扩展和完善：模块设计的理念(不同模块的爬虫)，深度，
2. redis：不同的存储方式解决对应的问题，对应场景选的不同的存储方式
3. webserver：操作系统资源的合理分配，数据结构、网络、操作系统结合深刻理解
4. 自己设计一个本地缓存
5. 后续暂定